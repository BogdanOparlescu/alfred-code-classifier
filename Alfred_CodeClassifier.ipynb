{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Alfred Code Classifier**"
      ],
      "metadata": {
        "id": "amry0lr-_a4p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dependency Installs and Imports"
      ],
      "metadata": {
        "id": "HVuCzHxT_hfY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Nchk8C4H_-Pi"
      },
      "outputs": [],
      "source": [
        "# Dependency Installs\n",
        "!pip install --upgrade pip\n",
        "!pip install transformers datasets tokenizers torch tqdm sentencepiece accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1tWbcvqdAIH2"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import os\n",
        "import math\n",
        "import random\n",
        "from pathlib import Path\n",
        "from tqdm.auto import tqdm\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "from datasets import load_dataset\n",
        "from tokenizers import Tokenizer, trainers, pre_tokenizers, models, processors, normalizers\n",
        "from transformers import PreTrainedTokenizerFast, AlbertConfig, AlbertForSequenceClassification, get_linear_schedule_with_warmup\n",
        "from torch.optim import AdamW\n",
        "\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "OUTPUT_DIR = Path(\"Alfred\")\n",
        "OUTPUT_DIR.mkdir(exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Downloading and Loading"
      ],
      "metadata": {
        "id": "Amp7UxlN_qQm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "3JS9xrZpAM_q"
      },
      "outputs": [],
      "source": [
        "# Downloading and Loading the Dataset\n",
        "dataset_name = \"burtenshaw/PleIAs_common_corpus_code_classification\"\n",
        "print(\"Loading dataset:\", dataset_name)\n",
        "ds = load_dataset(dataset_name)\n",
        "print(ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenizer and Batches Creation"
      ],
      "metadata": {
        "id": "rlBV6z_x_yPW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "E66QCWLcAeyF"
      },
      "outputs": [],
      "source": [
        "# Tokenizer Training\n",
        "from pathlib import Path\n",
        "vocab_size = 10000\n",
        "tokenizer_dir = Path(\"tokenizer\")\n",
        "tokenizer_dir.mkdir(exist_ok=True)\n",
        "\n",
        "def batch_iterator(split_name=\"train\", batch_size=1000):\n",
        "    for i in range(0, len(ds[split_name]), batch_size):\n",
        "        yield [str(x) for x in ds[split_name]['text'][i : i + batch_size]]\n",
        "\n",
        "if not (tokenizer_dir / \"tokenizer.json\").exists():\n",
        "    print(\"Training a WordPiece tokenizer on the dataset (vocab_size=%d)...\" % vocab_size)\n",
        "    wp_model = models.WordPiece(unk_token=\"[UNK]\")\n",
        "    tokenizer = Tokenizer(wp_model)\n",
        "    tokenizer.normalizer = normalizers.BertNormalizer(lowercase=True)\n",
        "    tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
        "    trainer = trainers.WordPieceTrainer(vocab_size=vocab_size, special_tokens=[\"[PAD]\",\"[UNK]\",\"[CLS]\",\"[SEP]\",\"[MASK]\"])\n",
        "    tokenizer.train_from_iterator(batch_iterator(\"train\"), trainer=trainer)\n",
        "    tokenizer.post_processor = processors.TemplateProcessing(\n",
        "        single=\"[CLS] $A [SEP]\",\n",
        "        pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
        "        special_tokens=[(\"[CLS]\", tokenizer.token_to_id(\"[CLS]\")), (\"[SEP]\", tokenizer.token_to_id(\"[SEP]\"))],\n",
        "    )\n",
        "    tokenizer.save(str(tokenizer_dir / \"tokenizer.json\"))\n",
        "    print(\"Tokenizer trained and saved to\", tokenizer_dir)\n",
        "else:\n",
        "    print(\"Tokenizer already exists, loading...\")\n",
        "    tokenizer = Tokenizer.from_file(str(tokenizer_dir / \"tokenizer.json\"))\n",
        "\n",
        "tok = PreTrainedTokenizerFast(tokenizer_object=tokenizer)\n",
        "tok.add_special_tokens({\"pad_token\":\"[PAD]\", \"unk_token\":\"[UNK]\", \"cls_token\":\"[CLS]\", \"sep_token\":\"[SEP]\", \"mask_token\":\"[MASK]\"})\n",
        "print(\"Vocab size:\", tok.vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vGRFfco1C-DB"
      },
      "outputs": [],
      "source": [
        "# Analyzing Labels\n",
        "all_labels = set()\n",
        "for split in ds.keys():\n",
        "    for lbl in ds[split]['labels']:\n",
        "        all_labels.add(lbl)\n",
        "\n",
        "all_labels = sorted(list(all_labels))\n",
        "print(\"Unique labels across full dataset:\", all_labels)\n",
        "\n",
        "label2id = {lbl: i for i, lbl in enumerate(all_labels)}\n",
        "id2label = {i: lbl for lbl, i in label2id.items()}\n",
        "num_labels = len(all_labels)\n",
        "\n",
        "print(\"num_labels =\", num_labels)\n",
        "print(\"label2id =\", label2id)\n",
        "\n",
        "def map_label_batch(batch):\n",
        "    return {'labels': [label2id[x] for x in batch['labels']]}\n",
        "\n",
        "ds = ds.map(map_label_batch, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C0sYcrgWDaFq"
      },
      "outputs": [],
      "source": [
        "# Expanding the dataset with a 256-token sliding window and stride of 128\n",
        "max_length = 256\n",
        "stride = 128\n",
        "\n",
        "def sliding_window_tokenize(examples):\n",
        "    batch_tokens = tok(\n",
        "        examples[\"text\"],\n",
        "        truncation=False,\n",
        "        padding=False,\n",
        "        add_special_tokens=True\n",
        "    )\n",
        "\n",
        "    all_input_ids = []\n",
        "    all_attention = []\n",
        "    all_labels = []\n",
        "\n",
        "    for input_ids, label in zip(batch_tokens[\"input_ids\"], examples[\"labels\"]):\n",
        "\n",
        "        total_len = len(input_ids)\n",
        "        start = 0\n",
        "\n",
        "        while True:\n",
        "            end = start + max_length\n",
        "\n",
        "            window_ids = input_ids[start:end]\n",
        "\n",
        "            all_input_ids.append(window_ids)\n",
        "            all_attention.append([1] * len(window_ids))\n",
        "            all_labels.append(label)\n",
        "\n",
        "            if end >= total_len:\n",
        "                break\n",
        "\n",
        "            start += stride\n",
        "\n",
        "    padded = tok.pad(\n",
        "        {\n",
        "            \"input_ids\": all_input_ids,\n",
        "            \"attention_mask\": all_attention\n",
        "        },\n",
        "        padding=\"max_length\",\n",
        "        max_length=max_length,\n",
        "        return_tensors=None\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"input_ids\": padded[\"input_ids\"],\n",
        "        \"attention_mask\": padded[\"attention_mask\"],\n",
        "        \"labels\": all_labels\n",
        "    }\n",
        "\n",
        "\n",
        "print(\"Tokenizing dataset with optimized sliding windows...\")\n",
        "\n",
        "keep_cols = [c for c in ds['train'].column_names if c not in ['labels']]\n",
        "\n",
        "tokenized = ds.map(\n",
        "    sliding_window_tokenize,\n",
        "    batched=True,\n",
        "    remove_columns=keep_cols\n",
        ")\n",
        "\n",
        "print(tokenized)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y7j72QGEIpuJ"
      },
      "outputs": [],
      "source": [
        "# Creating the batches for training\n",
        "def collate_fn(batch):\n",
        "    input_ids = [torch.tensor(b['input_ids'], dtype=torch.long) for b in batch]\n",
        "    attention_mask = [torch.tensor(b['attention_mask'], dtype=torch.long) for b in batch]\n",
        "    labels = torch.tensor([b['labels'] for b in batch], dtype=torch.long)\n",
        "    input_ids_padded = pad_sequence(input_ids, batch_first=True, padding_value=tok.pad_token_id)\n",
        "    attention_mask_padded = pad_sequence(attention_mask, batch_first=True, padding_value=0)\n",
        "    return {\"input_ids\": input_ids_padded, \"attention_mask\": attention_mask_padded, \"labels\": labels}\n",
        "\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(tokenized['train'], batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "if 'validation' in tokenized:\n",
        "    val_loader = DataLoader(tokenized['validation'], batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "else:\n",
        "    small_val = tokenized['train'].train_test_split(test_size=0.1, seed=seed)\n",
        "    train_loader = DataLoader(small_val['train'], batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "    val_loader = DataLoader(small_val['test'], batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "print(\"Batches - train:\", len(train_loader), \" val:\", len(val_loader))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining The Model Architecture"
      ],
      "metadata": {
        "id": "HQ-Ku1zz_8XW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cKGsIpEaJjYU"
      },
      "outputs": [],
      "source": [
        "# Model Architecture\n",
        "config = AlbertConfig(\n",
        "    vocab_size=11273, #hardcodded\n",
        "    embedding_size=64,\n",
        "    hidden_size=128,\n",
        "    num_hidden_layers=12,\n",
        "    num_attention_heads=4,\n",
        "    intermediate_size=512,\n",
        "    hidden_act=\"gelu\",\n",
        "    type_vocab_size=2,\n",
        "    layer_norm_eps=1e-12,\n",
        "    classifier_dropout_prob=0.1,\n",
        "    hidden_dropout_prob=0.1,\n",
        "    attention_probs_dropout_prob=0.1,\n",
        "    num_labels=155, #hardcodded\n",
        ")\n",
        "\n",
        "model = AlbertForSequenceClassification(config)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and Validation"
      ],
      "metadata": {
        "id": "1najpYm2__08"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kwkLxVd9LP0K"
      },
      "outputs": [],
      "source": [
        "# Training Loop\n",
        "epochs = 4\n",
        "total_steps = epochs * len(train_loader)\n",
        "optimizer = AdamW(model.parameters(), lr=2e-4)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=int(0.05*total_steps), num_training_steps=total_steps)\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "model.train()\n",
        "for epoch in range(1, epochs+1):\n",
        "    running_loss = 0.0\n",
        "    pbar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch}/{epochs}\")\n",
        "    for step, batch in pbar:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "        running_loss += loss.item()\n",
        "        if (step+1) % 10 == 0 or (step+1)==len(train_loader):\n",
        "            pbar.set_postfix({'loss': f'{running_loss/(step+1):.4f}'})\n",
        "    avg_train_loss = running_loss / len(train_loader)\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    preds = []\n",
        "    trues = []\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(val_loader, desc=\"Validation\", leave=False):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.logits\n",
        "            batch_preds = torch.argmax(logits, dim=-1).cpu().numpy().tolist()\n",
        "            preds.extend(batch_preds)\n",
        "            trues.extend(labels.cpu().numpy().tolist())\n",
        "    val_acc = accuracy_score(trues, preds)\n",
        "    print(f\"Epoch {epoch} finished. Train loss: {avg_train_loss:.4f} | Val accuracy: {val_acc:.4f}\")\n",
        "    model.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving and Loading the Model and Tokenizer"
      ],
      "metadata": {
        "id": "QDIz8UGQAEHq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ozD-_twWPEel"
      },
      "outputs": [],
      "source": [
        "# Model Saving\n",
        "print('Saving model and tokenizer to', OUTPUT_DIR)\n",
        "model.save_pretrained(OUTPUT_DIR)\n",
        "tok.save_pretrained(OUTPUT_DIR)\n",
        "print('Done.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zk_Cz_VR8wpJ"
      },
      "outputs": [],
      "source": [
        "!zip Alfred.zip CodeClassifier tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a4uCxzjVGg6L"
      },
      "outputs": [],
      "source": [
        "# Loading the Model and Tokenizer\n",
        "!unzip Alfred.zip\n",
        "tok = PreTrainedTokenizerFast.from_pretrained(OUTPUT_DIR)\n",
        "model = AlbertForSequenceClassification.from_pretrained(OUTPUT_DIR)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "print(\"Model and tokenizer loaded. Ready to resume training.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Confusiom Matrix for Training and Validation"
      ],
      "metadata": {
        "id": "7Al1vs9EAHfy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "uF5NK1n979P8"
      },
      "outputs": [],
      "source": [
        "# Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "model.eval()\n",
        "train_preds = []\n",
        "train_trues = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for step, batch in enumerate(tqdm(train_loader, desc=\"Training set predictions\")):\n",
        "      input_ids = batch['input_ids'].to(device)\n",
        "      attention_mask = batch['attention_mask'].to(device)\n",
        "      labels = batch['labels'].to(device)\n",
        "\n",
        "      outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "      logits = outputs.logits\n",
        "      batch_preds = torch.argmax(logits, dim=-1).cpu().numpy().tolist()\n",
        "      train_preds.extend(batch_preds)\n",
        "      train_trues.extend(labels.cpu().numpy().tolist())\n",
        "\n",
        "# Confusion matrix\n",
        "train_trues_str = [all_labels[i] for i in train_trues]\n",
        "train_preds_str = [all_labels[i] for i in train_preds]\n",
        "cm = confusion_matrix(train_trues_str, train_preds_str, labels=all_labels)\n",
        "cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(100, 100))\n",
        "sns.heatmap(cm_norm, annot=False, cmap='Blues', xticklabels=all_labels, yticklabels=all_labels)\n",
        "plt.xlabel('Predicted labels')\n",
        "plt.ylabel('True labels')\n",
        "plt.title('155x155 Confusion Matrix on Training Set')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}